{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict future vehicle speed from:\n",
    "\n",
    "Camera features (simulated CNN embeddings)\n",
    "\n",
    "LiDAR features (simulated point cloud embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:49:37.362606Z",
     "iopub.status.busy": "2026-01-04T14:49:37.361876Z",
     "iopub.status.idle": "2026-01-04T14:49:46.699110Z",
     "shell.execute_reply": "2026-01-04T14:49:46.698300Z",
     "shell.execute_reply.started": "2026-01-04T14:49:37.362579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nuscenes-devkit\n",
      "  Downloading nuscenes_devkit-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (6.2.1)\n",
      "Collecting descartes (from nuscenes-devkit)\n",
      "  Downloading descartes-1.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting fire (from nuscenes-devkit)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (3.7.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.4.58 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (4.12.0.88)\n",
      "Requirement already satisfied: Pillow>6.2.1 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (11.3.0)\n",
      "Collecting pyquaternion>=0.9.5 (from nuscenes-devkit)\n",
      "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (1.15.3)\n",
      "Collecting Shapely~=2.0.3 (from nuscenes-devkit)\n",
      "  Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (4.67.1)\n",
      "Collecting parameterized (from nuscenes-devkit)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from nuscenes-devkit) (2.0.10)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->nuscenes-devkit) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2.4.1)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python-headless>=4.5.4.58 (from nuscenes-devkit)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->nuscenes-devkit) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nuscenes-devkit) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nuscenes-devkit) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->nuscenes-devkit) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.22.0->nuscenes-devkit) (2024.2.0)\n",
      "Downloading nuscenes_devkit-1.2.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.0/316.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
      "Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading descartes-1.1.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Installing collected packages: parameterized, fire, Shapely, pyquaternion, opencv-python-headless, descartes, nuscenes-devkit\n",
      "  Attempting uninstall: Shapely\n",
      "    Found existing installation: shapely 2.1.2\n",
      "    Uninstalling shapely-2.1.2:\n",
      "      Successfully uninstalled shapely-2.1.2\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Shapely-2.0.7 descartes-1.1.0 fire-0.7.1 nuscenes-devkit-1.2.0 opencv-python-headless-4.11.0.86 parameterized-0.9.0 pyquaternion-0.9.9\n"
     ]
    }
   ],
   "source": [
    "!pip install nuscenes-devkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:49:46.700762Z",
     "iopub.status.busy": "2026-01-04T14:49:46.700522Z",
     "iopub.status.idle": "2026-01-04T14:52:03.422920Z",
     "shell.execute_reply": "2026-01-04T14:52:03.422232Z",
     "shell.execute_reply.started": "2026-01-04T14:49:46.700737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-04 14:49:46--  https://www.nuscenes.org/data/v1.0-mini.tgz\n",
      "Resolving www.nuscenes.org (www.nuscenes.org)... 13.227.219.18, 13.227.219.6, 13.227.219.45, ...\n",
      "Connecting to www.nuscenes.org (www.nuscenes.org)|13.227.219.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4167696325 (3.9G) [application/x-tar]\n",
      "Saving to: ‘v1.0-mini.tgz’\n",
      "\n",
      "v1.0-mini.tgz       100%[===================>]   3.88G  36.9MB/s    in 2m 16s  \n",
      "\n",
      "2026-01-04 14:52:03 (29.2 MB/s) - ‘v1.0-mini.tgz’ saved [4167696325/4167696325]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.nuscenes.org/data/v1.0-mini.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:52:03.424102Z",
     "iopub.status.busy": "2026-01-04T14:52:03.423860Z",
     "iopub.status.idle": "2026-01-04T14:52:44.890195Z",
     "shell.execute_reply": "2026-01-04T14:52:44.889358Z",
     "shell.execute_reply.started": "2026-01-04T14:52:03.424078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tar -xf v1.0-mini.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:52:44.892489Z",
     "iopub.status.busy": "2026-01-04T14:52:44.892227Z",
     "iopub.status.idle": "2026-01-04T14:52:46.157585Z",
     "shell.execute_reply": "2026-01-04T14:52:46.156916Z",
     "shell.execute_reply.started": "2026-01-04T14:52:44.892465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.499 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from nuscenes.nuscenes import NuScenes\n",
    "# Ensure you have added the nuScenes-mini dataset to your Kaggle inputs\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/kaggle/working', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:52:46.158778Z",
     "iopub.status.busy": "2026-01-04T14:52:46.158402Z",
     "iopub.status.idle": "2026-01-04T14:52:47.269127Z",
     "shell.execute_reply": "2026-01-04T14:52:47.268352Z",
     "shell.execute_reply.started": "2026-01-04T14:52:46.158754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene-0061, Parked truck, construction, intersectio... [18-07-24 03:28:47]   19s, singapore-onenorth, #anns:4622\n",
      "scene-0103, Many peds right, wait for turning car, ... [18-08-01 19:26:43]   19s, boston-seaport, #anns:2046\n",
      "scene-0655, Parking lot, parked cars, jaywalker, be... [18-08-27 15:51:32]   20s, boston-seaport, #anns:2332\n",
      "scene-0553, Wait at intersection, bicycle, large tr... [18-08-28 20:48:16]   20s, boston-seaport, #anns:1950\n",
      "scene-0757, Arrive at busy intersection, bus, wait ... [18-08-30 19:25:08]   20s, boston-seaport, #anns:592\n",
      "scene-0796, Scooter, peds on sidewalk, bus, cars, t... [18-10-02 02:52:24]   20s, singapore-queensto, #anns:708\n",
      "scene-0916, Parking lot, bicycle rack, parked bicyc... [18-10-08 07:37:13]   20s, singapore-queensto, #anns:2387\n",
      "scene-1077, Night, big street, bus stop, high speed... [18-11-21 11:39:27]   20s, singapore-hollandv, #anns:890\n",
      "scene-1094, Night, after rain, many peds, PMD, ped ... [18-11-21 11:47:27]   19s, singapore-hollandv, #anns:1762\n",
      "scene-1100, Night, peds in sidewalk, peds cross cro... [18-11-21 11:49:47]   19s, singapore-hollandv, #anns:935\n"
     ]
    }
   ],
   "source": [
    "nusc.list_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:52:47.270917Z",
     "iopub.status.busy": "2026-01-04T14:52:47.270707Z",
     "iopub.status.idle": "2026-01-04T14:52:48.814770Z",
     "shell.execute_reply": "2026-01-04T14:52:48.814008Z",
     "shell.execute_reply.started": "2026-01-04T14:52:47.270900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'cc8c0bf57f984915a77078b10eb33198',\n",
       " 'log_token': '7e25a2c8ea1f41c5b0da1e69ecfa71a2',\n",
       " 'nbr_samples': 39,\n",
       " 'first_sample_token': 'ca9a282c9e77460f8360f564131a8af5',\n",
       " 'last_sample_token': 'ed5fc18c31904f96a8f0dbb99ff069c0',\n",
       " 'name': 'scene-0061',\n",
       " 'description': 'Parked truck, construction, intersection, turn left, following a van'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_scene = nusc.scene[0]\n",
    "my_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:52:48.815768Z",
     "iopub.status.busy": "2026-01-04T14:52:48.815568Z",
     "iopub.status.idle": "2026-01-04T14:52:49.256832Z",
     "shell.execute_reply": "2026-01-04T14:52:49.256207Z",
     "shell.execute_reply.started": "2026-01-04T14:52:48.815752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': '725903f5b62f56118f4094b46a4470d8',\n",
       "  'channel': 'CAM_FRONT',\n",
       "  'modality': 'camera'},\n",
       " {'token': 'ce89d4f3050b5892b33b3d328c5e82a3',\n",
       "  'channel': 'CAM_BACK',\n",
       "  'modality': 'camera'},\n",
       " {'token': 'a89643a5de885c6486df2232dc954da2',\n",
       "  'channel': 'CAM_BACK_LEFT',\n",
       "  'modality': 'camera'},\n",
       " {'token': 'ec4b5d41840a509984f7ec36419d4c09',\n",
       "  'channel': 'CAM_FRONT_LEFT',\n",
       "  'modality': 'camera'},\n",
       " {'token': '2f7ad058f1ac5557bf321c7543758f43',\n",
       "  'channel': 'CAM_FRONT_RIGHT',\n",
       "  'modality': 'camera'},\n",
       " {'token': 'ca7dba2ec9f95951bbe67246f7f2c3f7',\n",
       "  'channel': 'CAM_BACK_RIGHT',\n",
       "  'modality': 'camera'},\n",
       " {'token': 'dc8b396651c05aedbb9cdaae573bb567',\n",
       "  'channel': 'LIDAR_TOP',\n",
       "  'modality': 'lidar'},\n",
       " {'token': '47fcd48f71d75e0da5c8c1704a9bfe0a',\n",
       "  'channel': 'RADAR_FRONT',\n",
       "  'modality': 'radar'},\n",
       " {'token': '232a6c4dc628532e81de1c57120876e9',\n",
       "  'channel': 'RADAR_FRONT_RIGHT',\n",
       "  'modality': 'radar'},\n",
       " {'token': '1f69f87a4e175e5ba1d03e2e6d9bcd27',\n",
       "  'channel': 'RADAR_FRONT_LEFT',\n",
       "  'modality': 'radar'},\n",
       " {'token': 'df2d5b8be7be55cca33c8c92384f2266',\n",
       "  'channel': 'RADAR_BACK_LEFT',\n",
       "  'modality': 'radar'},\n",
       " {'token': '5c29dee2f70b528a817110173c2e71b9',\n",
       "  'channel': 'RADAR_BACK_RIGHT',\n",
       "  'modality': 'radar'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nusc.sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:26.253313Z",
     "iopub.status.busy": "2026-01-04T14:53:26.253035Z",
     "iopub.status.idle": "2026-01-04T14:53:26.281751Z",
     "shell.execute_reply": "2026-01-04T14:53:26.281051Z",
     "shell.execute_reply.started": "2026-01-04T14:53:26.253293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total synchronized samples found: 394\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# List all 6 camera channels available in nuScenes\n",
    "CAMERA_NAMES = ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', \n",
    "                'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT']\n",
    "\n",
    "def extract_full_system_data(nusc):\n",
    "    dataset_index = []\n",
    "    for sample in nusc.sample:\n",
    "        # Collect all 6 camera paths into a dictionary\n",
    "        cam_paths = {name: nusc.get_sample_data_path(sample['data'][name]) for name in CAMERA_NAMES}\n",
    "        lidar_path = nusc.get_sample_data_path(sample['data']['LIDAR_TOP'])\n",
    "        \n",
    "        # Calculate velocity using ego-pose (your existing logic)\n",
    "        cam_token = sample['data']['CAM_FRONT']\n",
    "        current_pose = nusc.get('ego_pose', nusc.get('sample_data', cam_token)['ego_pose_token'])\n",
    "        prev_sample_token = sample['prev']\n",
    "        if prev_sample_token == '': continue\n",
    "        prev_sample = nusc.get('sample', prev_sample_token)\n",
    "        prev_pose = nusc.get('ego_pose', nusc.get('sample_data', prev_sample['data']['CAM_FRONT'])['ego_pose_token'])\n",
    "        \n",
    "        time_diff = (current_pose['timestamp'] - prev_pose['timestamp']) / 1e6\n",
    "        dist = np.linalg.norm(np.array(current_pose['translation']) - np.array(prev_pose['translation']))\n",
    "        velocity = dist / time_diff if time_diff > 0 else 0\n",
    "        \n",
    "        dataset_index.append({\n",
    "            'cameras': cam_paths,\n",
    "            'lidar_path': lidar_path,\n",
    "            'speed': velocity\n",
    "        })\n",
    "    return dataset_index\n",
    "\n",
    "sync_data = extract_full_system_data(nusc)\n",
    "\n",
    "print(f\"Total synchronized samples found: {len(sync_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Lidar raw points to birds eye view .**\n",
    "\n",
    "\n",
    "Converts nuScenes LiDAR bin file to a 2-channel BEV map.\n",
    "boundary: (min_x, max_x, min_y, max_y) in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:32.043693Z",
     "iopub.status.busy": "2026-01-04T14:53:32.043171Z",
     "iopub.status.idle": "2026-01-04T14:53:32.050672Z",
     "shell.execute_reply": "2026-01-04T14:53:32.049854Z",
     "shell.execute_reply.started": "2026-01-04T14:53:32.043671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lidar_bev(bin_path, grid_size=(224, 224), boundary=(-50, 50, -50, 50)):\n",
    "    # 1. Load and slice points correctly\n",
    "    scan = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 5)\n",
    "    points = scan[:, :3] \n",
    "\n",
    "    # 2. Filter points within boundary\n",
    "    mask = (points[:, 0] >= boundary[0]) & (points[:, 0] < boundary[1]) & \\\n",
    "           (points[:, 1] >= boundary[2]) & (points[:, 1] < boundary[3])\n",
    "    points = points[mask]\n",
    "\n",
    "    # 3. Map meters to pixel indices\n",
    "    x_px = ((points[:, 0] - boundary[0]) / (boundary[1] - boundary[0]) * grid_size[0]).astype(np.int32)\n",
    "    y_px = ((points[:, 1] - boundary[2]) / (boundary[3] - boundary[2]) * grid_size[1]).astype(np.int32)\n",
    "\n",
    "    # 4. Create BEV channels (Fast Vectorized Way)\n",
    "    bev_map = np.zeros((2, grid_size[0], grid_size[1]), dtype=np.float32)\n",
    "    \n",
    "    # Normalize Z (height)\n",
    "    norm_z = np.clip(points[:, 2] + 2, 0, 5) / 5.0\n",
    "\n",
    "    # Instead of a loop, we use 'at' to handle multiple points hitting the same pixel\n",
    "    # Channel 0: Max Height\n",
    "    np.maximum.at(bev_map[0], (x_px, y_px), norm_z)\n",
    "    \n",
    "    # Channel 1: Density (Point Count)\n",
    "    np.add.at(bev_map[1], (x_px, y_px), 1)\n",
    "\n",
    "    # Normalize Density\n",
    "    if bev_map[1].max() > 0:\n",
    "        bev_map[1] = np.log1p(bev_map[1]) / np.log1p(bev_map[1].max())\n",
    "\n",
    "    return torch.from_numpy(bev_map)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:34.260494Z",
     "iopub.status.busy": "2026-01-04T14:53:34.260225Z",
     "iopub.status.idle": "2026-01-04T14:53:34.267464Z",
     "shell.execute_reply": "2026-01-04T14:53:34.266727Z",
     "shell.execute_reply.started": "2026-01-04T14:53:34.260475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data  import Dataset , DataLoader \n",
    "from PIL import Image \n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class NuScenesFusion(Dataset):\n",
    "    def __init__(self, sync_data):\n",
    "\n",
    "        self.data = sync_data\n",
    "        self.cam_transform = T.Compose([\n",
    "            T.Resize((224,224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # 1. Load all 6 images\n",
    "        image_list = []\n",
    "        for name in CAMERA_NAMES:\n",
    "            img = Image.open(item['cameras'][name]).convert('RGB')\n",
    "            image_list.append(self.cam_transform(img))\n",
    "        \n",
    "        # Make sure this name is used in the return statement!\n",
    "        full_camera_tensor = torch.stack(image_list)\n",
    "        \n",
    "        # 2. Load LiDAR\n",
    "        bev_tensor = lidar_bev(item['lidar_path'])\n",
    "        \n",
    "        # 3. Target Speed\n",
    "        speed = torch.tensor([item['speed']], dtype=torch.float32)\n",
    "        \n",
    "        # FIX: Ensure these match the variables defined above\n",
    "        return full_camera_tensor, bev_tensor, speed\n",
    "\n",
    "dataset = NuScenesFusion(sync_data)\n",
    "train_loader = DataLoader(dataset , batch_size=8 , shuffle = True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:39.008625Z",
     "iopub.status.busy": "2026-01-04T14:53:39.007845Z",
     "iopub.status.idle": "2026-01-04T14:53:39.015018Z",
     "shell.execute_reply": "2026-01-04T14:53:39.014398Z",
     "shell.execute_reply.started": "2026-01-04T14:53:39.008599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "class SurroundVelocityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Shared Camera Backbone: processes each image independently\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.cam_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # LiDAR Branch\n",
    "        self.lidar_branch = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))     \n",
    "        )\n",
    "\n",
    "        # Regressor: 512 features per camera * 6 cameras + 32 lidar features\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear((512 * 6) + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cams, bev):\n",
    "        # cams shape: (Batch, 6, 3, 224, 224)\n",
    "        b, n, c, h, w = cams.shape\n",
    "        \n",
    "        # Flatten batch and cameras to process through backbone at once\n",
    "        cams = cams.view(-1, c, h, w) \n",
    "        cam_feats = self.cam_backbone(cams).view(b, n, -1) # (B, 6, 512)\n",
    "        \n",
    "        # Concatenate features from all 6 cameras\n",
    "        cam_feats_combined = cam_feats.view(b, -1) # (B, 3072)\n",
    "        \n",
    "        # Process LiDAR\n",
    "        lidar_feat = self.lidar_branch(bev).view(b, -1)\n",
    "        \n",
    "        # Final Fusion\n",
    "        combined = torch.cat((cam_feats_combined, lidar_feat), dim=1)\n",
    "        return self.regressor(combined)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:58.686964Z",
     "iopub.status.busy": "2026-01-04T14:53:58.686316Z",
     "iopub.status.idle": "2026-01-04T14:53:58.871833Z",
     "shell.execute_reply": "2026-01-04T14:53:58.870999Z",
     "shell.execute_reply.started": "2026-01-04T14:53:58.686929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera Batch Shape: torch.Size([2, 6, 3, 224, 224])\n",
      "LiDAR Batch Shape: torch.Size([2, 2, 224, 224])\n",
      "Speed Batch Shape: torch.Size([2, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/1787664270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Test the model forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bevs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model Output Shape: {test_output.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Expected: [2, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Grab one batch to test shapes\n",
    "test_loader = DataLoader(dataset, batch_size=2)\n",
    "test_cams, test_bevs, test_speeds = next(iter(test_loader))\n",
    "\n",
    "print(f\"Camera Batch Shape: {test_cams.shape}\") # Expected: [2, 6, 3, 224, 224]\n",
    "print(f\"LiDAR Batch Shape: {test_bevs.shape}\")  # Expected: [2, 2, 224, 224]\n",
    "print(f\"Speed Batch Shape: {test_speeds.shape}\") # Expected: [2, 1]\n",
    "\n",
    "# Test the model forward pass\n",
    "test_output = model(test_cams.to(device), test_bevs.to(device))\n",
    "print(f\"Model Output Shape: {test_output.shape}\") # Expected: [2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T14:53:59.567488Z",
     "iopub.status.busy": "2026-01-04T14:53:59.566936Z",
     "iopub.status.idle": "2026-01-04T15:01:07.700343Z",
     "shell.execute_reply": "2026-01-04T15:01:07.699646Z",
     "shell.execute_reply.started": "2026-01-04T14:53:59.567468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 218MB/s]\n",
      "Epoch [1/10]: 100%|██████████| 50/50 [00:43<00:00,  1.15it/s, loss=3.64]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete. Avg Loss: 1.6538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete. Avg Loss: 0.5645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.715] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete. Avg Loss: 0.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.0938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete. Avg Loss: 0.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.24]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete. Avg Loss: 0.2166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.372] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Complete. Avg Loss: 0.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.383] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Complete. Avg Loss: 0.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.00538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Complete. Avg Loss: 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, loss=0.0101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Complete. Avg Loss: 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 50/50 [00:42<00:00,  1.18it/s, loss=0.0541] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Complete. Avg Loss: 0.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SurroundVelocityNet().to(device)\n",
    "\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_model(model , train_loader , epochs=10):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader , total=len(train_loader))\n",
    "        epoch_loss = 0\n",
    "\n",
    "\n",
    "        for cams, bevs, speeds in loop: # 'cams' now contains the 6-camera stack\n",
    "            cams = cams.to(device)      # Shape: (Batch, 6, 3, 224, 224)\n",
    "            bevs = bevs.to(device)\n",
    "            speeds = speeds.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(cams, bevs)\n",
    "            loss = criterion(predictions, speeds)\n",
    "            \n",
    "            # Backward pass & Optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Complete. Avg Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:01:07.701670Z",
     "iopub.status.busy": "2026-01-04T15:01:07.701450Z",
     "iopub.status.idle": "2026-01-04T15:01:07.816663Z",
     "shell.execute_reply": "2026-01-04T15:01:07.815673Z",
     "shell.execute_reply.started": "2026-01-04T15:01:07.701652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- A. Save the Model Weights ---\n",
    "torch.save(model.state_dict(), 'K_fusion_model.pth')\n",
    "print(\"Model saved as 'K_fusion_model.pth'\")\n",
    "\n",
    "# --- B. Plot 1: Training Convergence (Loss Curve) ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history, color='#3498db', label='Huber Loss')\n",
    "plt.title('Fusion Model Training Convergence', fontsize=16)\n",
    "plt.xlabel('Iterations', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig('loss_curve.png') # Saves plot as image\n",
    "plt.show()\n",
    "\n",
    "# --- C. Plot 2: Prediction Dashboard (The \"Fixed\" Version) ---\n",
    "def final_dashboard(model, dataset, device):\n",
    "    model.eval()\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    image_tensor, bev_tensor, true_speed = dataset[idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Handle batch dimension\n",
    "        pred_speed = model(image_tensor.unsqueeze(0).to(device), \n",
    "                           bev_tensor.unsqueeze(0).to(device))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Image View (Handle both 1-cam and 6-cam)\n",
    "    # If 4D (6, 3, 224, 224), take the first camera\n",
    "    display_img = image_tensor[0] if image_tensor.dim() == 4 else image_tensor\n",
    "    img_show = display_img.permute(1, 2, 0).cpu().numpy()\n",
    "    img_show = (img_show * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
    "    \n",
    "    ax[0].imshow(np.clip(img_show, 0, 1))\n",
    "    ax[0].set_title(\"On-Board Camera View\", fontsize=14)\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # 2. LiDAR BEV\n",
    "    ax[1].imshow(bev_tensor[1], cmap='magma')\n",
    "    ax[1].set_title(\"LiDAR Bird's Eye View (Density)\", fontsize=14)\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"GROUND TRUTH: {true_speed.item():.2f} m/s  |  AI PREDICTION: {pred_speed.item():.2f} m/s\", \n",
    "                 fontsize=18, fontweight='bold', y=1.05, color='#2c3e50')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_visual.png')\n",
    "    plt.show()\n",
    "\n",
    "# Run the dashboard\n",
    "final_dashboard(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:02:08.112746Z",
     "iopub.status.busy": "2026-01-04T15:02:08.112156Z",
     "iopub.status.idle": "2026-01-04T15:02:20.524198Z",
     "shell.execute_reply": "2026-01-04T15:02:20.523452Z",
     "shell.execute_reply.started": "2026-01-04T15:02:08.112723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching 6-Camera system into video...\n",
      "Video saved to surround_prediction.mp4!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def generate_surround_video(model, dataset, device, output_path='surround_prediction.mp4'):\n",
    "    model.eval()\n",
    "    # Define video properties: 1200x600 (3 images wide, 2 high)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 10.0, (1200, 600))\n",
    "    \n",
    "    print(\"Stitching 6-Camera system into video...\")\n",
    "\n",
    "    # Process first 100 samples for the video\n",
    "    for i in range(min(100, len(dataset))):\n",
    "        cams, bev, speed = dataset[i]\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            pred = model(cams.unsqueeze(0).to(device), bev.unsqueeze(0).to(device)).item()\n",
    "        \n",
    "        # Process 6 images into a grid\n",
    "        frames = []\n",
    "        for j in range(6):\n",
    "            # Convert tensor back to CV2 format (Un-normalize)\n",
    "            img = cams[j].permute(1, 2, 0).cpu().numpy()\n",
    "            img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
    "            img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img = cv2.resize(img, (400, 300))\n",
    "            frames.append(img)\n",
    "            \n",
    "        # Stitch: Top (FL, F, FR) | Bottom (BL, B, BR)\n",
    "        top = np.hstack(frames[:3])\n",
    "        bottom = np.hstack(frames[3:])\n",
    "        grid = np.vstack([top, bottom])\n",
    "        \n",
    "        # Add HUD Text\n",
    "        color = (0, 255, 0) if abs(pred - speed.item()) < 1.0 else (0, 165, 255)\n",
    "        cv2.putText(grid, f\"AI PREDICTION: {pred:.2f} m/s\", (400, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "        cv2.putText(grid, f\"GROUND TRUTH: {speed.item():.2f} m/s\", (400, 580), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(grid)\n",
    "        \n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path}!\")\n",
    "\n",
    "# Run this to generate your 6D Video\n",
    "generate_surround_video(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:04:13.129872Z",
     "iopub.status.busy": "2026-01-04T15:04:13.129353Z",
     "iopub.status.idle": "2026-01-04T15:04:32.449771Z",
     "shell.execute_reply": "2026-01-04T15:04:32.448996Z",
     "shell.execute_reply.started": "2026-01-04T15:04:13.129848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def generate_K_presentation(model, dataset, device, output_path='K_presentation.mp4'):\n",
    "    model.eval()\n",
    "    # 3x3 Grid: 1200x900 (Each block is 400x300)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 10.0, (1200, 900))\n",
    "    \n",
    "    # Mapping dataset indices to visual positions\n",
    "    # Index in dataset image stack: 0:F, 1:FR, 2:BR, 3:B, 4:BL, 5:FL\n",
    "    pos_map = {\n",
    "        (0, 0): 5, # Top-Left: Front-Left\n",
    "        (0, 1): 0, # Top-Mid: Front\n",
    "        (0, 2): 1, # Top-Right: Front-Right\n",
    "        (1, 0): 4, # Mid-Left: Back-Left\n",
    "        (1, 2): 2, # Mid-Right: Back-Right\n",
    "        (2, 1): 3  # Bot-Mid: Back\n",
    "    }\n",
    "\n",
    "    print(\"Rendering K Command Center Video...\")\n",
    "    for i in tqdm(range(min(150, len(dataset)))):\n",
    "        cams, bev, speed = dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = model(cams.unsqueeze(0).to(device), bev.unsqueeze(0).to(device)).item()\n",
    "\n",
    "        # Create Canvas\n",
    "        canvas = np.zeros((900, 1200, 3), dtype=np.uint8)\n",
    "        \n",
    "        # 1. Place 6 Cameras\n",
    "        for (row, col), cam_idx in pos_map.items():\n",
    "            img = cams[cam_idx].permute(1, 2, 0).cpu().numpy()\n",
    "            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "            img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img = cv2.resize(img, (400, 300))\n",
    "            canvas[row*300:(row+1)*300, col*400:(col+1)*400] = img\n",
    "\n",
    "        # 2. Place LiDAR in Center (Row 1, Col 1)\n",
    "        # Use a cool colormap (Magma) for the LiDAR density channel\n",
    "        lidar_img = bev[1].cpu().numpy()\n",
    "        lidar_color = (cm.magma(lidar_img)[:, :, :3] * 255).astype(np.uint8)\n",
    "        lidar_color = cv2.cvtColor(lidar_color, cv2.COLOR_RGB2BGR)\n",
    "        lidar_color = cv2.resize(lidar_color, (400, 300))\n",
    "        \n",
    "        # Add a \"Tech\" border to LiDAR\n",
    "        cv2.rectangle(lidar_color, (0,0), (399,299), (0, 255, 255), 3)\n",
    "        cv2.putText(lidar_color, \"LIDAR BEV FUSION\", (10, 280), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        canvas[300:600, 400:800] = lidar_color\n",
    "\n",
    "        # 3. Add HUD Info (Bottom Left and Bottom Right)\n",
    "        # Draw tech-y background boxes for text\n",
    "        cv2.rectangle(canvas, (20, 800), (380, 880), (30, 30, 30), -1)\n",
    "        cv2.rectangle(canvas, (820, 800), (1180, 880), (30, 30, 30), -1)\n",
    "        \n",
    "        # Speed Prediction\n",
    "        color = (0, 255, 0) if abs(pred - speed.item()) < 0.8 else (0, 150, 255)\n",
    "        cv2.putText(canvas, f\"AI PREDICT: {pred:.2f} m/s\", (40, 850), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "        \n",
    "        # Ground Truth\n",
    "        cv2.putText(canvas, f\"TRUE SPEED: {speed.item():.2f} m/s\", (840, 850), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        # Add a scanline effect or timestamp for \"coolness\"\n",
    "        cv2.putText(canvas, f\"FRAME: {i:03d} | SYSTEM: ACTIVE\", (450, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        out.write(canvas)\n",
    "        \n",
    "    out.release()\n",
    "    print(f\"Presentation video saved to {output_path}\")\n",
    "\n",
    "# Run the final export\n",
    "generate_K_presentation(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-04T15:04:34.142362Z",
     "iopub.status.busy": "2026-01-04T15:04:34.142068Z",
     "iopub.status.idle": "2026-01-04T15:04:34.146627Z",
     "shell.execute_reply": "2026-01-04T15:04:34.145992Z",
     "shell.execute_reply.started": "2026-01-04T15:04:34.142339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_history, label='Training Loss (Huber)')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Fusion Model Convergence')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
