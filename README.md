# Autonomous-vehicle-MultiSensor-Fusion
360Â° Multi-Modal Velocity Fusion for Autonomous VehiclesðŸ“Œ Project OverviewThis project implements a high-performance Mid-Fusion Neural Network designed to predict vehicle velocity by integrating data from a 360Â° surround-view camera suite (6 cameras) and LiDAR Bird's Eye View (BEV).Unlike single-sensor systems, this model performs spatial synchronization across 7 asynchronous sensors to build a comprehensive "world model," enabling accurate ego-motion estimation even in complex urban environments.ðŸš€ Key FeaturesSurround-View Perception: Processes 6 simultaneous camera feeds (Front, Front-Left, Front-Right, Back, Back-Left, Back-Right) using a shared ResNet-18 backbone.LiDAR Voxelization: Transforms raw .bin point clouds into 2-channel BEV maps (Height and Density) using vectorized NumPy operations for sub-millisecond preprocessing.Feature-Level Fusion: Implements a mid-fusion architecture that concatenates semantic visual features with geometric LiDAR data.Real-Time Dashboard: Includes a "Command Center" visualization suite that stitches sensor data into a 360Â° HUD.ðŸ›  Tech StackFramework: PyTorch, TorchVisionData Handling: NuScenes SDK, NumPy, PILVisualization: OpenCV, MatplotlibOptimization: Huber Loss (Robust to outliers in velocity ground truth)ðŸ“Š Model ArchitectureThe model consists of two main branches:Vision Branch: A Time-Distributed ResNet-18 encoder that extracts a 512-dimensional feature vector from each of the 6 cameras.LiDAR Branch: A 2D Convolutional network that processes the BEV grid to extract spatial occupancy features.Regressor: A fully connected head that maps the 3,104 combined features to a scalar velocity (m/s).ðŸš¦ Results & VisualizationThe model successfully converges using Huber Loss, demonstrating robust performance against noisy LiDAR returns.Surround-View PerceptionLiDAR BEV Mapping(Note: Replace the placeholders above with links to your actual exported images/videos in your GitHub repo)ðŸ“‚ Project StructurePlaintextâ”œâ”€â”€ data/                  # nuScenes-mini dataset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fusion_main.ipynb  # Core training and inference logic
â”œâ”€â”€ models/
â”‚   â””â”€â”€ kodiak_fusion.pth  # Trained model weights
â”œâ”€â”€ outputs/               # Saved visualizations and HUD videos
â””â”€â”€ README.md
ðŸ§  Engineering Highlights (The "Kodiak" Edge)Spatial Synchronization: Handled extrinsic calibration offsets to ensure objects passing between camera fields-of-view maintained feature consistency.Latency Optimization: Replaced iterative Python loops with vectorized np.maximum.at operations for LiDAR projection, reducing data-loading bottlenecks by 10x.Robustness: Utilized Huber Loss to mitigate the impact of ego-pose drift in the nuScenes metadata during high-acceleration frames.ðŸ›  Installation & UsageClone the repo: git clone https://github.com/your-username/av-velocity-fusion.gitInstall dependencies: pip install -r requirements.txtDownload nuScenes-mini and place it in the data/ folder.Run the training notebook or the inference script to generate the dashboard.
